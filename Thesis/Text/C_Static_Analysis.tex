\chapter{Static Analysis}
\label{chap:StaticAnalysis}

In ''Secure Programming with Static Analysis'' \textbf{static analysis} is defined as followed:

\begin{quotation}
The term static analysis refers to any process for assessing code without
executing it. Static analysis is powerful because it allows for the quick consideration of many possibilities. A static analysis tool can explore a large number of ''what if'' scenarios without having to go through all the computations
necessary to execute the code for all the scenarios.\footnote{\citep[3]{SecureProgramming}}
\end{quotation}

Another definition that highlights the use of prediction during the process of analysis can be found in ''Principles of program analysis'':

\begin{quotation}
Program analysis offers static compile-time techniques for predicting safe and computable approximations to the set of values or behaviours arising dynamically at run-time when executing a program on a computer.\footnote{\citep[1]{ProgramAnalysis}}
\end{quotation}

Thus, tools performing static analysis evaluate software in the abstract, without running the software or considering a specific input.\footnote{\citep[1]{UsingSAToFindBugs}}

Complementary to static program analysis, there is \textbf{dynamic program analysis}, which analyses software at run-time. Unit testing comes to mind as a prominent example of dynamic program analysis.

\newpage
\section{Area of application}
\label{sec:AreaOfApplication}

The general purpose of static analysis is to extract useful information from source code or compiled code (like byte code or binary code).

The type of information retrieved depends on the context and purpose of the static analysis and can be arranged into different categories:

\begin{itemize}\addtolength{\itemsep}{-0.5\baselineskip}
\item Program correctness
\item Software security
\item Style checking
\item Program understanding
\item Optimization
\end{itemize}

\subsection{Program correctness}

Static analysis tools that are concerned with program correctness try to detect defects at compile-time so that those do not manifest in run-time errors, such as crashes, data corruption and program malfunctioning. Thus these tools, which are mostly concerned with \textbf{software quality}, are often referred to as \textbf{bug finders}.

Bug finders point out places in the source code, where the program will behave in a way that the programmer did not intend. Most tools are easy to use because they come pre-stocked with a set of ''bug idioms'' (rules) that describe patterns in code that often indicate bugs.\footnote{\citep[32]{SecureProgramming}} They help to find these often hard-to-spot defects early in the software development life-cycle, reducing the cost, time, and risk of software errors.\footnote{\citep{CovertySA}}

A prominent example of preventing run-time errors at compile-time is type checking. In this sense, compilers themselves can be seen as static analysis tools and - as we will see later - both have much in common.

''The lint program for C programs is generally considered the first widely used static analysis tool for defect detection.''\footnote{\citep[1]{UsingSAToFindBugs}}

\subsubsection{Generic and context-specific defects}

Defects can be categorized in generic and context-specific defects.

\textbf{Generic defects} are problems that can occur in almost any program written in the given programming language, such as buffer overflows and memory leaks.\\
\textbf{Context-specific defects} on the other hand require specific knowledge about the semantics of the given program.\footnote{\citep[14]{SecureProgramming}} Applications that deal with atomic operations (like a program that handles money transfer) belong to the latter category.

The following sections will present different kinds of bugs, most of which can be found by analysing the control and data flow of an application.

\subsubsection{Overflow and range analysis problems}

A buffer overflow occurs when a program writes data outside the bounds of allocated memory\footnote{\citep[175]{SecureProgramming}}, for example when writing data into a buffer that is too small for the data to be written, leading to the memory after the buffer being overwritten and thus corrupted. Static analysis tools can detect such problems and advice the programmer to do a bounds-check.

An integer overflow occurs when an integral value is increased or decreased beyond its capacity\footnote{\citep[235]{SecureProgramming}}. The following example demonstrates how this can lead to an infinite loop:

\SingleSpacing
\begin{lstlisting}[language=C++, caption=Integer ''underflow'' in C++]
for(unsigned int i = 200; i >= 0; --i)
{
	// ...
}
\end{lstlisting}
\OnehalfSpacing

A static analysis tool will detect that the expression \textbf{i \textgreater= 0} will always be true for \textbf{unsigned} integer types like \textbf{i}, as i will be set to the highest possible unsigned integer when being 0 and decreased by 1.

\subsubsection{Resource leaks}

Resource leaks can be detected by tracking request (for example allocation of memory) and release of resources.

If a program gives up all references to a resource while it is has not been released, the resource is leaked. On the other hand, using a resource that has not been requested properly or releasing an already released resource may lead to unexpected results or even crashes.

The most prominent example of a resource leak is a memory leak. But there are other important resources like files and databases that may need to be locked before usage and released afterwards.

\subsubsection{Threads and concurrency}

Static analysis tools can also be used to find problems with threads such as data races and deadlocks.

Similar to the detection of resource leaks, the data and call flow of the application can be analysed to track the locking and unlocking of semaphores.

\subsubsection{Other problems and warnings}

There are many more possible sources for bugs that can be detected using static analysis tools, such as division by zero, dereferencing of null-pointers or the returning of references or pointers to local function variables.

Bug finders are not only concerned with finding malicious code that will lead to run-time errors, but may also warn about code that is redundant, like comparisons that will always have the same result. Although such a comparison won't cause a failure or exception, its existence suggests that it might have resulted from a coding error, leading to incorrect program behaviour.\footnote{\citep[1]{UsingSAToFindBugs}}

Similar warnings could be issued for functions whose return values (for example error codes) have not been checked or for exception catch-clauses that are too broad, so that they catch important exceptions like OutOfMemoryError without handling them.

\subsection{Software security}

\begin{quotation}
Static analysis is particularly well suited to security because many security problems occur in corner cases and hard-to-reach states that can be difficult to exercise by actually running the code.\footnote{\citep[4]{SecureProgramming}}
\end{quotation}

Concerning software security, static analysis tools are used to find coding errors before they can be exploited. Buffer overflows and format string vulnerabilities come to mind, possibly resulting in SQL injection, cross-site scripting or unwarranted acquisition of administrator privileges. 

Static analysis may also track input data flow and validation to determine
all the implicit ways a program might be putting unwarranted faith in some aspect of its input.\footnote{\citep[172]{SecureProgramming}}

\subsection{Style checking}

Style checkers enforce rules related to whitespace, position of scope-brackets, naming conventions, deprecated functions, commenting, program structure, and similar non-semantic issues.\footnote{\citep[25]{SecureProgramming}}

\todo{picture}

\subsection{Program understanding}

Program understanding tools help users make sense of large codebases. \footnote{\citep[27]{SecureProgramming}} Most modern IDE's provide functionality for helping the user navigate the codebase, for example jumping to a function's definition or finding all uses of a method. Some even provide functionality for refactoring symbols, like renaming classes. All these operations need knowledge about the symbols existing in the codebase.

Other tools visualize the source code, for example by creating UML diagrams from source, rendering images of class hierarchies or function callgraphs that can help the user understand the control flow of the application and the relationships between the existing symbols.

\todo{picture}

Prominent tools like Doxygen\footnote{\url{http://www.stack.nl/~dimitri/doxygen/}} automatically create documentation from source code and code comments.

\subsection{Optimization}

Compilers use static analysis to perform code optimization:

\begin{quotation}
A main application is to allow compilers to generate code avoiding redundant computations, e.g. by reusing available results or by moving loop invariant computations out of loops, or avoiding superfluous computations, e.g. of results known to be not needed or of results known already at compile-time.\footnote{\citep[1]{ProgramAnalysis}}
\end{quotation}

Apart from code optimization, compile-time optimization, for example by finding unnecessary include-files in C++ applications is another area that static analysis is used for. \textbf{include-what-you-use}\footnote{\url{http://code.google.com/p/include-what-you-use/}} is a tool to perform this task. It internally uses the Clang compiler.

\section{Static analysis as part of a programmers workflow}

Being major factors of software quality, companies put a lot of effort into improving and verifying \textbf{program correctness} and \textbf{software security}. Different techniques have been developed, each concerned with achieving this main goal, but taking varying routes and targeting different stages of the development.

\textbf{Software engineering} and planning begin long before any program code is written, specifying program behaviour and design, and finding structural problems and security concerns before implementation. \textbf{Pair programming} aims at reducing coding errors and making design choices at the time of writing program code by using human experience and communication. \textbf{Code review} is similar, but happens at a later stage. \textbf{Dynamic testing}, including unit tests, has been invented to verify correct run-time program behaviour of written code programmatically, with the help of more program code that tests implementations and compares the results to the expected output.

All these techniques follow a defensive mindset about writing applications, knowing that every programmer, no matter how experienced, will make mistakes that lead to undefined program behaviour. All in all these techniques aim at reducing the rate of errors that go unnoticed.

\textbf{Static analysis} is another technique in the programmers toolbox serving this goal, during and after code creation. Compared to dynamic testing and code review, it reduces the human factor, programatically applying ''checks thoroughly and consistently, without any of the bias that a programmer might have about which pieces of code are 'interesting' (...) or which pieces of code are easy to exercise through dynamic testing.''\footnote{\citep[22]{SecureProgramming}} Static analysis tools accumulate the knowledge of experienced programmers and as Brian Chess and Jacob West point out, the feedback given by these tools acts as a means of knowledge transfer by showing the programmer types of errors he was not aware of.\footnote{\citep[22]{SecureProgramming}} In this way, a "static analysis tool can make the code review process faster and more fruitful by hypothesizing a set of potential problems for consideration during a code review."\footnote{\citep[13]{SecureProgramming}}

As with dynamic testing and code review, static analysis can not verify the absence of errors, but is a technique to reduce the amount of bugs before release.

\subsection{Drawbacks}

As every technique, static analysis has drawbacks. 

Depending on how well a tool works and how good it is balanced, static analysis tools may produce a lot of \textbf{false positives}, meaning pointing out an error where none exists. Too much ''noise'' is problematic, as it costs the programmer valuable time to check if a result is really a bug or not. Also, real problems may be overseen, when false positives dominate the tools output.

\textbf{False negatives}, on the other hand, are real bugs that a tool did not discover. It has already been said, that complete absence of errors can not be proven with any tool - so, using static analysis tools may give the programmer a false sense of security.\footnote{\citep[23]{SecureProgramming}}


\section{Static analysis tool internals}

To retrieve useful information from the code, static analysis tools need to build an internal representation (model) of the program to be analysed. The structure of that model heavily depends on the purpose of the tool and has to make good trade-offs between precision, depth, and scalability.\footnote{\citep[45]{SecureProgramming}} After such a model has been created, the tool can use the information for further processing. It could, for example, perform rule-based checks on the model to find coding errors.

\subsection{The model}

When creating the internal program model, static analysis tools generally borrow a lot from the compiler world\footnote{\citep[72]{SecureProgramming}}. They especially follow the first two phases of a compiler: lexical analysis and syntax analysis.

\begin{quotation}
\textbf{Lexical analysis} (...) is the initial part of reading and analysing the program text: The text is read and divided into \textit{tokens}, each of which corresponds to a symbol in the programming language, e.g., a variable name, keyword or number.\footnote{\citep[2]{CompilerBasics}}
\end{quotation}

Compared to compilers, static analysis tools may filter out less during lexical analysis. They may preserve comments, in case the tool needs information from comments for the model (Doxygen is a prominent example for this case).
Static analysis tools that search for coding errors usually also associate the tokens with their source code positions for later error reporting.\footnote{\citep[72]{SecureProgramming}} 

\begin{quotation}
\textbf{Syntax analysis} (...) takes the list of tokens produced by the lexical analysis and arranges these in a tree-structure (called the \textit{syntax tree}) that reflects the structure of the program. This phase is often called \textit{parsing}.\footnote{\citep[2]{CompilerBasics}}
\end{quotation}

As the created syntax tree contains a lot of irrelevant data that is not needed for later analysis (parentheses, etc.), the syntax tree is usually transformed into an \textbf{abstract syntax tree (AST)}, where such information is either removed or combined from multiple tree nodes into single ones.\footnote{\citep[99]{CompilerBasics}}

Some static analysis tools may go their own way from this point on and create their model based on the abstract syntax tree.\\Other tools, like bug finders, as well as compilers for statically typed languages will perform another phase common in the compiler world: \textbf{semantic analysis}. During that phase, a symbol table will be build, which associates each identifier found in the program with its type and a pointer to its declaration or definition.\footnote{\citep[76]{SecureProgramming}} Using the created table, the tool or compiler is able to perform type checking to check the correctness of the program.\footnote{\citep[76]{SecureProgramming}}

Bug finders additionally need to populate their model with information about the effects of library or system calls invoked by the program being analysed.\footnote{\citep[37]{SecureProgramming}}

\subsection{Applying rules}

After creating a model, static analysis tools that check for correctness, will perform checks on the model using a set of rules that are delivered with the tool. These rules may search for common ''bug idioms'' like memory leaks, buffer overflows or may define the characteristics of a special security related problem.

Some tools allow the user to extend the set of rules to search for bugs that may be specific to the user's codebase or that are not found by the rules delivered with the tool.\footnote{\citep[97]{SecureProgramming}}

For altering the output of the static analysis tool, some bug finders allow the annotation of source code passages.\footnote{\citep[99]{SecureProgramming}} An annotation can be helpful to suppress a false positive in the output.
\\For languages that do not provide special syntax for annotations, as Java and C\# do, annotations usually
take the form of specially formatted comments.\footnote{\citep[99]{SecureProgramming}}

The approaches that are used for checking rules differ from rule to rule. Some checks track control flow or data flow throughout the program. Others use alternative relationships that can be drawn from the model. How static analysis tools perform these checks is out of scope of this thesis. For further knowledge the reader is advised to gather information about \textit{Control Flow Analysis}, \textit{Data Flow Analysis}, \textit{Constraint Based Analysis}, \textit{Taint Propagation}, \textit{Abstract Interpretation}, \textit{Interprocdural Analysis }as well as \textit{Type and Effect Systems}. 

\section{Existing static analysis tools}

The application to be developed will make use of a 3rd party library for performing an integral part of the analysis of C++ headers. 

The most important factor of the chosen static analysis tool/library is its ability to parse C++ and to export or give access to a (simplified) abstract syntax tree of the parsed source files, including symbol and type information. Information about namespaces, global variables, functions including parameters and return types, classes, class-members and more needs to be extracted.

Also, as this project is meant to be open-source, its dependencies must be released in an according licence, which excludes all commercial static analysis tools like Coverty\footnote{\url{http://www.coverity.com/}}, PVS-Studio\footnote{\url{http://www.viva64.com/en/pvs-studio/}} or pc-lint\footnote{\url{http://www.gimpel.com/html/pcl.htm}}. It is also not known, whether these tools would provide the ability to export symbol and type information.

Further, the chosen library needed to be in active development to adjust to changes coming in the new C++ standard. This excluded a couple of small open source projects that dealt with parsing C++ headers.

The choice had to be made between three projects: GCC-XML, Dehydra and Clang.

\subsection{GCC-XML}

GCC-XML\footnote{\url{http://www.gccxml.org/HTML/Index.html}} is an extension to GCC's C++ parser that outputs AST information into an XML file. It does not seem to provide any means of filtering the output. The developed application would need to parse the XML and build its own representation internally. Hence, GCC-XML only gives indirect access to GCC's syntax tree and is thus not well-suited for solving the problem.

\subsection{Dehydra}

\begin{quotation}
Dehydra is a lightweight, scriptable, general purpose static analysis tool capable of application-specific analyses of C++ code (...) [that] is built as a GCC plugin, (...).\footnote{\citep{MDNDehydra}}
\end{quotation}

Dehydra is developed by Mozilla for checking specific bug patterns in its large codebase for Firefox and other Mozilla applications. It allows custom querying of the AST with the help of JavaScript scripts.

When compiling a project with GCC and Dehydra, the user provides a script file along with the command-line arguments. As the compilation happens, functions in the given script are called to inform about found types, functions and other symbols. This way the script can build its own AST representation, filtering symbols on the fly when necessary.

An example function that a script may provide is ''process\_type(t)'', which is called by Dehydra whenever a new type was found in the compiling code.

The biggest drawback of Dehydra is that it is currently only supported on Linux based operating systems.

Dehydra was used for implementing DXR\footnote{\url{https://github.com/mozilla/dxr}}, Mozilla's web source code cross-reference tool, that can be used to navigate inside the Mozilla code base. DXR development moved to using Clang in newer builds, though.

\subsection{Clang and libclang}

\begin{quotation}
Clang is an ''LLVM native'' C/C++/Objective-C compiler, which aims to deliver amazingly fast compiles (...), extremely useful error and warning messages and to provide a platform for building great source level tools. The Clang Static Analyzer is a tool that automatically finds bugs in your code, and is a great example of the sort of tool that can be built using the Clang frontend as a library to parse C/C++ code.\footnote{\citep{LLVMHP}}
\end{quotation}

''The LLVM Project is a collection of modular and reusable compiler and toolchain technologies''. The LLVM compiler framework includes multiple compiler front-ends, like Clang for the C language family, an optimizer and code generation support for many popular CPUs.\footnote{\citep{LLVMHP}}

The open-source Clang project is not only concerned with developing a compiler that produces high-performance code, but also with developing software that can be used in utilities like IDE's and bug finders to perform source code analysis and indexing.

There are two approaches of how an application can integrate Clang for analysing purposes.
\\On the one hand, one can include Clang itself as a library and use the internal structures and functions of the compiler, thus having access to the AST build by Clang in a direct way.
\\The second approach is to use \textbf{libclang}, a C-API to Clang, which exposes much of Clang's functionality for indexing source code and accessing ASTs, while maintaining a stable interface independent of changes internal to Clang. libclang is extended on a per-need-basis and may thus not provide all features needed for a static analysis tool that builds upon it. As it is an open-source project, these features can easily be added.

Clang works on all popular operating systems, including Microsoft Windows. Having this advantage over Dehydra, Clang is the favourite choice for use in this project. A detailed description of how libclang is used inside the application can be found in \myRefChapter{chap:AnalysingCPP}.

\subsubsection{Emscripten}

In the context of this thesis, another interesting LLVM-based project should be mentioned. ''Emscripten is an LLVM-to-JavaScript compiler. It takes LLVM bitcode (which can be generated from C/C++ using Clang, or any other language that can be converted into LLVM bitcode) and compiles that into JavaScript, which can be run on the web.''\footnote{\citep{EmscriptenHP} - taken from front page} Instead of accessing C++ through glue code, the C++ code itself is completely compiled to JavaScript. This may sound very slow in the first place, but with the current performance of JavaScript virtual machines and the reduced cost of type-conversion (as all is JavaScript), code generated by Emscripten is only 3 to 4 times slower than native code generated with gcc -O3.\footnote{\citep{EmscriptenHP} - taken from FAQ section}